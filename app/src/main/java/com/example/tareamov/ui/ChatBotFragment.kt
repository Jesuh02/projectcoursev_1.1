package com.example.tareamov.ui

import android.os.Bundle
import android.util.Log
import android.view.LayoutInflater
import android.view.View
import android.view.ViewGroup
import android.widget.*
import androidx.fragment.app.Fragment
import androidx.lifecycle.lifecycleScope
import androidx.navigation.fragment.findNavController
import androidx.recyclerview.widget.LinearLayoutManager
import androidx.recyclerview.widget.RecyclerView
import com.example.tareamov.R
import com.example.tareamov.data.AppDatabase
import com.example.tareamov.data.entity.ChatMessage
import com.example.tareamov.data.entity.FileContext
import com.example.tareamov.service.AIAnalysisService
import com.example.tareamov.service.FileAnalysisService
import com.example.tareamov.ui.adapter.ChatMessageAdapter
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.delay
import kotlinx.coroutines.launch
import kotlinx.coroutines.withContext
import java.util.*

class ChatBotFragment : Fragment() {

    private lateinit var messagesRecyclerView: RecyclerView
    private lateinit var messageEditText: EditText
    private lateinit var sendButton: ImageButton
    private lateinit var backButton: ImageButton
    private lateinit var clearChatButton: ImageButton
    private lateinit var loadingProgressBar: ProgressBar
    
    private lateinit var chatAdapter: ChatMessageAdapter
    private lateinit var database: AppDatabase
    private lateinit var aiAnalysisService: AIAnalysisService
    private lateinit var fileAnalysisService: FileAnalysisService
    
    private val sessionId = UUID.randomUUID().toString()
    private var currentFileContext: FileContext? = null

    override fun onCreateView(
        inflater: LayoutInflater, container: ViewGroup?,
        savedInstanceState: Bundle?
    ): View? {
        return inflater.inflate(R.layout.fragment_chatbot, container, false)
    }

    override fun onViewCreated(view: View, savedInstanceState: Bundle?) {
        super.onViewCreated(view, savedInstanceState)
        
        database = AppDatabase.getDatabase(requireContext())
        aiAnalysisService = AIAnalysisService(requireContext())
        fileAnalysisService = FileAnalysisService(requireContext())
        
        initializeViews(view)
        setupRecyclerView()
        setupClickListeners()
        loadMessages()
        loadFileContextFromArguments()
        
        // Probar conexi√≥n con Ollama al iniciar
        testOllamaConnectionOnStart()
    }

    private fun initializeViews(view: View) {
        messagesRecyclerView = view.findViewById(R.id.messagesRecyclerView)
        messageEditText = view.findViewById(R.id.messageEditText)
        sendButton = view.findViewById(R.id.sendButton)
        backButton = view.findViewById(R.id.backButton)
        clearChatButton = view.findViewById(R.id.clearChatButton)
        loadingProgressBar = view.findViewById(R.id.loadingProgressBar)
        
        database = AppDatabase.getDatabase(requireContext())
    }

    private fun loadFileContextFromArguments() {
        arguments?.let { args ->
            val submissionId = args.getLong("submissionId", -1L)
            val errorMessage = args.getString("errorMessage")
            val fileName = args.getString("fileName")
            
            if (errorMessage != null) {
                // Mostrar mensaje de error del archivo
                lifecycleScope.launch {
                    val errorChatMessage = ChatMessage(
                        message = "‚ö†Ô∏è **Error con el archivo**\n\n" +
                                "üìÅ Archivo: ${fileName ?: "desconocido"}\n" +
                                "‚ùå Error: $errorMessage\n\n" +
                                "üí¨ Puedes seguir usando el chat, pero sin el contexto completo del archivo.\n" +
                                "Para obtener mejor ayuda, intenta subir el archivo localmente.",
                        isFromUser = false,
                        sessionId = sessionId
                    )
                    
                    withContext(Dispatchers.IO) {
                        database.chatMessageDao().insertMessage(errorChatMessage)
                    }
                }
                
                // A pesar del error, intentamos cargar el contexto si existe
                if (submissionId != -1L) {
                    loadFileContextById(submissionId, true)
                }
                
                return
            }
            
            if (submissionId != -1L) {
                loadFileContextById(submissionId, false)
            }
        }
    }
    
    /**
     * Carga el contexto del archivo por ID y muestra mensajes apropiados
     */
    private fun loadFileContextById(submissionId: Long, hasError: Boolean) {
        lifecycleScope.launch {
            currentFileContext = withContext(Dispatchers.IO) {
                database.fileContextDao().getFileContextBySubmission(submissionId)
            }
            
            if (currentFileContext != null) {
                // Verificar si es un error espec√≠fico de Google Drive
                val isGoogleDriveError = currentFileContext!!.fileType == "google_drive_error"
                
                // Mostrar mensaje inicial con contexto del archivo
                val contextMessage = if (isGoogleDriveError) {
                    ChatMessage(
                        message = "üì± **Archivo de Google Drive detectado**\n\n" +
                                "üìÑ Nombre: ${currentFileContext!!.fileName}\n" +
                                "‚ö†Ô∏è **No se puede acceder directamente a este archivo**\n\n" +
                                "Para poder analizar este archivo, necesitas:\n" +
                                "1. Abrir Google Drive\n" +
                                "2. Descargar el archivo a tu dispositivo\n" +
                                "3. Volver a subir el archivo desde tu almacenamiento local\n\n" +
                                "Mientras tanto, puedo ayudarte con preguntas generales.",
                        isFromUser = false,
                        sessionId = sessionId
                    )
                } else if (hasError) {
                    ChatMessage(
                        message = "üìÑ **Archivo parcialmente accesible**\n\n" +
                                "üìÅ Nombre: ${currentFileContext!!.fileName}\n" +
                                "üîß Tipo: ${currentFileContext!!.fileType}\n" +
                                "‚ö†Ô∏è El archivo tiene problemas de acceso, pero intentar√© ayudarte con la informaci√≥n disponible.\n\n" +
                                "Puedes hacerme preguntas y har√© lo mejor posible con los datos limitados.",
                        isFromUser = false,
                        sessionId = sessionId
                    )
                } else {
                    ChatMessage(
                        message = "üìÅ **Archivo cargado exitosamente**\n\n" +
                                "üìÑ Nombre: ${currentFileContext!!.fileName}\n" +
                                "üîß Tipo: ${currentFileContext!!.fileType}\n" +
                                "üìä Contenido: ${currentFileContext!!.fileContent.length} caracteres\n\n" +
                                "‚úÖ Puedes hacerme preguntas sobre este archivo y te ayudar√© con el an√°lisis.",
                        isFromUser = false,
                        sessionId = sessionId
                    )
                }
                
                withContext(Dispatchers.IO) {
                    database.chatMessageDao().insertMessage(contextMessage)
                }
            }
        }
    }

    private fun setupRecyclerView() {
        chatAdapter = ChatMessageAdapter()
        messagesRecyclerView.apply {
            adapter = chatAdapter
            layoutManager = LinearLayoutManager(context).apply {
                stackFromEnd = true // Start from bottom
            }
        }
    }

    private fun setupClickListeners() {
        backButton.setOnClickListener {
            findNavController().navigateUp()
        }

        sendButton.setOnClickListener {
            sendMessage()
        }

        clearChatButton.setOnClickListener {
            clearChat()
        }

        messageEditText.setOnEditorActionListener { _, _, _ ->
            sendMessage()
            true
        }
    }

    private fun loadMessages() {
        lifecycleScope.launch {
            database.chatMessageDao().getAllMessages().collect { messages ->
                chatAdapter.submitList(messages) {
                    // Scroll to bottom when new messages are added
                    if (messages.isNotEmpty()) {
                        messagesRecyclerView.smoothScrollToPosition(messages.size - 1)
                    }
                }
            }
        }
    }

    private fun sendMessage() {
        val messageText = messageEditText.text.toString().trim()
        if (messageText.isEmpty()) return

        lifecycleScope.launch {
            // Clear input field
            messageEditText.text.clear()
            
            // Save user message
            val userMessage = ChatMessage(
                message = messageText,
                isFromUser = true,
                sessionId = sessionId
            )
            
            withContext(Dispatchers.IO) {
                database.chatMessageDao().insertMessage(userMessage)
            }
            
            // Show loading indicator
            loadingProgressBar.visibility = View.VISIBLE
            
            try {
                // Generate AI response using Ollama
                val botResponse = withContext(Dispatchers.IO) {
                    // Usar la funci√≥n que prueba m√∫ltiples modelos
                    tryMultipleModels(
                        userMessage = messageText,
                        fileContext = currentFileContext
                    )
                }
                
                val botMessage = ChatMessage(
                    message = botResponse,
                    isFromUser = false,
                    sessionId = sessionId
                )
                
                withContext(Dispatchers.IO) {
                    database.chatMessageDao().insertMessage(botMessage)
                }
                
            } catch (e: Exception) {
                // Fallback response si hay error
                val errorMessage = ChatMessage(
                    message = "Lo siento, tuve un problema al procesar tu mensaje. ${generateFallbackResponse(messageText)}",
                    isFromUser = false,
                    sessionId = sessionId
                )
                
                withContext(Dispatchers.IO) {
                    database.chatMessageDao().insertMessage(errorMessage)
                }
            } finally {
                // Hide loading indicator
                loadingProgressBar.visibility = View.GONE
            }
        }
    }

    private fun generateFallbackResponse(userMessage: String): String {
        // Simple bot responses - you can enhance this with actual AI integration
        return when {
            userMessage.contains("hola", ignoreCase = true) || 
            userMessage.contains("buenos d√≠as", ignoreCase = true) ||
            userMessage.contains("buenas tardes", ignoreCase = true) -> {
                "¬°Hola! Soy tu asistente virtual. ¬øEn qu√© puedo ayudarte con tus tareas y estudios?"
            }
            userMessage.contains("tarea", ignoreCase = true) -> {
                "Puedo ayudarte con informaci√≥n sobre tus tareas. ¬øQu√© necesitas saber espec√≠ficamente?"
            }
            userMessage.contains("calificaci√≥n", ignoreCase = true) ||
            userMessage.contains("nota", ignoreCase = true) -> {
                "Para consultar tus calificaciones, revisa la secci√≥n de entregas en cada curso. ¬øHay alguna calificaci√≥n espec√≠fica que te preocupe?"
            }
            userMessage.contains("curso", ignoreCase = true) -> {
                "Puedo proporcionarte informaci√≥n sobre los cursos disponibles. ¬øQu√© curso te interesa?"
            }
            userMessage.contains("ayuda", ignoreCase = true) -> {
                "Estoy aqu√≠ para ayudarte. Puedo responder preguntas sobre:\n‚Ä¢ Tareas y entregas\n‚Ä¢ Calificaciones\n‚Ä¢ Cursos disponibles\n‚Ä¢ Navegaci√≥n en la aplicaci√≥n\n\n¬øQu√© necesitas?"
            }
            userMessage.contains("gracias", ignoreCase = true) -> {
                "¬°De nada! Estoy aqu√≠ cuando me necesites. ¬øHay algo m√°s en lo que pueda ayudarte?"
            }
            userMessage.contains("adi√≥s", ignoreCase = true) ||
            userMessage.contains("hasta luego", ignoreCase = true) -> {
                "¬°Hasta luego! Que tengas un excelente d√≠a de estudios. üìö"
            }
            else -> {
                val responses = listOf(
                    "Entiendo tu consulta. ¬øPodr√≠as ser m√°s espec√≠fico para poder ayudarte mejor?",
                    "Interesante pregunta. Te sugiero que revises la documentaci√≥n del curso o contactes a tu profesor para m√°s detalles.",
                    "Puedo ayudarte con eso. ¬øPodr√≠as proporcionarme m√°s contexto sobre lo que necesitas?",
                    "Esa es una buena pregunta. Te recomiendo explorar los recursos del curso o buscar en la biblioteca digital.",
                    "Para obtener la mejor respuesta, te sugiero que consultes con tu instructor o revises el material del curso."
                )
                responses.random()
            }
        }
    }

    private fun clearChat() {
        lifecycleScope.launch {
            withContext(Dispatchers.IO) {
                database.chatMessageDao().clearAllMessages()
            }
            Toast.makeText(context, "Chat limpiado", Toast.LENGTH_SHORT).show()
        }
    }

    private fun testOllamaConnectionOnStart() {
        lifecycleScope.launch {
            try {
                Log.d("ChatBotFragment", "üîç Iniciando prueba de conexi√≥n con Ollama...")
                
                // Limpiar cache para forzar nuevos intentos
                aiAnalysisService.clearEndpointCache()
                
                // Obtener endpoints detectados para mostrar informaci√≥n
                val detectedEndpoints = aiAnalysisService.getDetectedEndpoints()
                Log.d("ChatBotFragment", "üì° Endpoints detectados: ${detectedEndpoints.size}")
                
                val connectionResult = withContext(Dispatchers.IO) {
                    aiAnalysisService.testOllamaConnection()
                }
                val (serverConnected, graniteAvailable) = connectionResult
                
                val currentEndpoint = aiAnalysisService.getCurrentEndpoint()
                
                val statusMessage = if (serverConnected && currentEndpoint != null) {
                    val modelStatus = if (graniteAvailable) {
                        "‚úÖ Modelo Granite disponible"
                    } else {
                        "‚ö†Ô∏è Modelo Granite NO disponible - Ejecuta: ollama run granite-code"
                    }
                    
                    Log.d("ChatBotFragment", "‚úÖ Conexi√≥n exitosa con Ollama en: $currentEndpoint, Granite disponible: $graniteAvailable")
                    "ü§ñ **Asistente de IA Activado**\n\n" +
                    "‚úÖ Conectado a Ollama exitosamente\n" +
                    "üåê Endpoint: `$currentEndpoint`\n" +
                    "üì° Endpoints detectados: ${detectedEndpoints.size}\n" +
                    "üß† Modelo: `granite-code`\n" +
                    "$modelStatus\n" +
                    "üí¨ Listo para analizar archivos y responder preguntas\n\n" +
                    (if (!graniteAvailable) "‚ö†Ô∏è **IMPORTANTE:** Algunas funciones estar√°n limitadas hasta que instales el modelo requerido.\n\n" else "") +
                    "¬øEn qu√© puedo ayudarte hoy?"
                } else {
                    Log.w("ChatBotFragment", "‚ö†Ô∏è No se pudo conectar con Ollama")
                    val endpointsList = detectedEndpoints.take(3).joinToString("\n") { "‚Ä¢ `$it`" }
                    val moreText = if (detectedEndpoints.size > 3) "\n‚Ä¢ ... y ${detectedEndpoints.size - 3} m√°s" else ""
                    
                    "ü§ñ **Asistente de IA (Modo B√°sico)**\n\n" +
                    "‚ö†Ô∏è No se pudo conectar con Ollama\n" +
                    "üì° Endpoints probados:\n$endpointsList$moreText\n\n" +
                    "üìù Funcionando con respuestas predefinidas\n" +
                    "üîß Verifica que Ollama est√© ejecut√°ndose y que el modelo 'granite-code' est√© instalado\n" +
                    "üíª Para instalar el modelo, ejecuta: ollama run granite-code\n\n" +
                    "¬øEn qu√© puedo ayudarte?"
                }
                
                // Agregar mensaje de estado del sistema
                val systemMessage = ChatMessage(
                    message = statusMessage,
                    isFromUser = false,
                    sessionId = sessionId
                )
                
                withContext(Dispatchers.IO) {
                    database.chatMessageDao().insertMessage(systemMessage)
                }
                
            } catch (e: Exception) {
                Log.e("ChatBotFragment", "‚ùå Error probando conexi√≥n con Ollama", e)
                
                val errorMessage = ChatMessage(
                    message = "ü§ñ **Error de Conexi√≥n**\n\n" +
                            "‚ùå Error al conectar con el servicio de IA\n" +
                            "üìù Funcionando en modo b√°sico\n" +
                            "üîß Revisa la configuraci√≥n de red\n\n" +
                            "Error: ${e.message}",
                    isFromUser = false,
                    sessionId = sessionId
                )
                
                withContext(Dispatchers.IO) {
                    database.chatMessageDao().insertMessage(errorMessage)
                }
            }
        }
    }

    /**
     * Intenta generar una respuesta utilizando diferentes modelos de Ollama en caso de error
     */
    /**
     * Intenta generar una respuesta con m√∫ltiples modelos de IA, fallback en caso de error
     */
    private suspend fun tryMultipleModels(userMessage: String, fileContext: FileContext? = null): String {
        // Verificar primero si Granite est√° disponible, si no, mostrar mensaje de instalaci√≥n
        val (serverConnected, graniteAvailable) = aiAnalysisService.testOllamaConnection()
        
        if (!serverConnected) {
            return "‚ö†Ô∏è **Error de conexi√≥n con Ollama**\n\n" +
                   "No se pudo conectar al servidor Ollama. Por favor verifica que:\n\n" +
                   "1. El servidor Ollama est√© ejecut√°ndose\n" +
                   "2. El puerto 11434 est√© abierto y accesible\n" +
                   "3. La conexi√≥n de red entre la app y el servidor funcione correctamente\n\n" +
                   "Ejecuta el siguiente comando para iniciar Ollama:\n" +
                   "```\nollama serve\n```"
        }
        
        if (!graniteAvailable) {
            return "‚ö†Ô∏è **Modelo Granite no encontrado**\n\n" +
                   "El modelo requerido '**granite-code**' no est√° instalado.\n\n" +
                   "Por favor inst√°lalo con el siguiente comando:\n" +
                   "```\nollama run granite-code\n```\n\n" +
                   "Esta aplicaci√≥n est√° dise√±ada para funcionar √≥ptimamente con el modelo Granite y no " +
                   "utilizar√° otros modelos como alternativa."
        }
        
        // Si Granite est√° disponible, intentar usarlo
        try {
            Log.d("ChatBotFragment", "Usando el modelo Granite")
            return aiAnalysisService.analyzeWithContext(
                userMessage = userMessage,
                fileContext = fileContext,
                model = "granite-code"
            )
        } catch (e: Exception) {
            Log.e("ChatBotFragment", "Error con modelo Granite: ${e.message}")
            
            // Si el error es espec√≠ficamente "modelo no encontrado", mostrar mensaje de instalaci√≥n
            if (e.message?.contains("not found") == true || e.message?.contains("404") == true) {
                return "‚ö†Ô∏è **Modelo Granite no encontrado**\n\n" +
                       "El modelo '**granite-code**' no est√° disponible en el servidor Ollama.\n\n" +
                       "Por favor inst√°lalo con el siguiente comando:\n" +
                       "```\nollama run granite-code\n```\n\n" +
                       "Esta aplicaci√≥n est√° dise√±ada para funcionar exclusivamente con este modelo."
            }
            
            // Para otros errores, generar respuesta de fallback
            return "Lo siento, tuve un problema al procesar tu mensaje. Error: ${e.message}\n\n" +
                   generateFallbackResponse(userMessage)
        }
    }
}
