# Configuraci√≥n de Ollama para la App Android

## ‚ö†Ô∏è REQUISITO OBLIGATORIO - MODELO GRANITE ‚ö†Ô∏è

Esta aplicaci√≥n REQUIERE el modelo IBM Granite para funcionar correctamente:

```
ibm-granite/granite-3b-code-instruct-128k
```

Si ves este error: `{"error":"model 'ibm-granite/granite-3b-code-instruct-128k' not found"}`, significa que el modelo no est√° instalado.

### Instalaci√≥n del modelo IBM Granite:

```bash
# Aseg√∫rate de que Ollama est√© ejecut√°ndose y luego ejecuta:
ollama pull ibm-granite/granite-3b-code-instruct-128k
```

> üìå IMPORTANTE: La aplicaci√≥n est√° dise√±ada para usar EXCLUSIVAMENTE el modelo IBM Granite y NO usar√° otros modelos como fallback.

Para una f√°cil verificaci√≥n, usa el script incluido:
- Windows: `VERIFICAR_MODELO_GRANITE.bat`
- Linux/Mac: `VERIFICAR_MODELO_GRANITE.sh`

## ‚ö†Ô∏è SOLUCI√ìN PARA ERRORES DE CONEXI√ìN

### Si ves error: "failed to connect to /10.0.2.2 (port 11434)"

#### 1. Verificar que Ollama est√© corriendo:

```bash
# Abrir PowerShell como administrador y ejecutar:
$env:OLLAMA_HOST = '0.0.0.0:11434'
ollama serve
```

#### 2. Verificar que el puerto est√© abierto:

```bash
# En otra ventana de PowerShell:
netstat -an | findstr 11434
# Deber√≠as ver: TCP 0.0.0.0:11434 0.0.0.0:0 LISTENING
```

#### 3. Probar con curl desde tu PC:

```bash
# Probar el endpoint:
curl http://localhost:11434/api/generate -d '{"model":"llama3.2","prompt":"test","stream":false}'
```

#### 4. Configurar Firewall de Windows:

```bash
# Ejecutar como administrador:
netsh advfirewall firewall add rule name="Ollama" dir=in action=allow protocol=TCP localport=11434
```

#### 5. Para dispositivo f√≠sico (no emulador):

1. Encuentra tu IP local:
```bash
ipconfig
# Busca la IP de tu adaptador de red (ej: 192.168.1.117)
```

2. En el dispositivo Android, la app detectar√° autom√°ticamente la IP correcta entre:
   - `http://10.0.2.2:11434/` (emulador)
   - `http://localhost:11434/` (localhost)
   - `http://127.0.0.1:11434/` (loopback)
   - `http://192.168.1.117:11434/` (tu IP local)

## Pasos para configurar Ollama:

### 1. Instalar y configurar Ollama

```bash
# Descargar e instalar Ollama desde https://ollama.com/
# En Windows:
# Ejecutar como administrador en PowerShell:

# Configurar Ollama para aceptar conexiones externas
$env:OLLAMA_HOST = '0.0.0.0:11434'  # IMPORTANTE: usar 0.0.0.0, no localhost
ollama serve
```

### 2. Descargar modelo de IA

```bash
# En otra terminal/PowerShell:
ollama pull llama3.2
```

### 3. Verificar que funcione

```bash
# Probar el modelo:
ollama run llama3.2
# Escribir una pregunta de prueba como "Hola, ¬øc√≥mo est√°s?"
```

### 4. Verificar conectividad

1. Abrir la app y ir al ChatBot
2. La app probar√° autom√°ticamente m√∫ltiples direcciones
3. Deber√≠a mostrar "‚úÖ Conectado a Ollama exitosamente"
4. Si aparece "‚ö†Ô∏è No se pudo conectar", verificar:
   - Ollama est√° ejecut√°ndose con `$env:OLLAMA_HOST = '0.0.0.0:11434'`
   - El firewall permite conexiones en el puerto 11434
   - No hay otros servicios usando el puerto 11434

### 5. Modelos recomendados

- `ibm-granite/granite-3b-code-instruct-128k` - **Modelo REQUERIDO para chat** 
- `llama3.2` - Modelo general balanceado (alternativa)
- `llama3.2:1b` - Modelo m√°s peque√±o y r√°pido (alternativa)
- `codellama` - Especializado en c√≥digo (alternativa)

```bash
# IMPORTANTE: Para la funcionalidad completa de la app, descargar:
ollama pull ibm-granite/granite-3b-code-instruct-128k

# Para descargar modelos alternativos:
ollama pull llama3.2:1b
ollama pull codellama
```

## ‚ö†Ô∏è SOLUCI√ìN PARA ERRORES DE ARCHIVOS

### Si ves error: "StorageFileLoadException[connection_failure]"

Esto significa que el archivo est√° en Google Drive y no se puede acceder directamente.

#### Soluciones:

1. **Descargar archivo localmente**: Descarga el archivo de Google Drive a tu dispositivo
2. **Usar archivos locales**: Sube archivos directamente desde el almacenamiento local
3. **Copiar y pegar**: Copia el contenido del archivo y p√©galo como texto en el chat

### Troubleshooting

#### Error de conexi√≥n:
1. Verificar que Ollama est√© corriendo: `ollama list`
2. Verificar puerto: `netstat -an | findstr 11434`
3. Verificar logs en Android Studio Logcat buscando "AIAnalysisService"
4. **IMPORTANTE**: Usar `0.0.0.0:11434` no `localhost:11434`

#### Respuestas lentas:
- Usar modelos m√°s peque√±os como `llama3.2:1b`
- Verificar recursos de sistema (RAM/GPU)

#### Sin GPU:
```bash
# Forzar uso de CPU:
$env:OLLAMA_CPU_ONLY = 'true'
ollama serve
```

#### Verificaci√≥n completa:
```bash
# Ejecutar estos comandos paso a paso:

# 1. Configurar variable de entorno
$env:OLLAMA_HOST = '0.0.0.0:11434'

# 2. Iniciar Ollama
ollama serve

# 3. En otra ventana, verificar modelos
ollama list

# 4. Verificar puerto abierto
netstat -an | findstr 11434

# 5. Probar API
curl http://localhost:11434/api/generate -d '{"model":"llama3.2","prompt":"test","stream":false}'
```
